{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune few layers of LLM + WandB integration\n",
    "\n",
    "In this notebook, we will follow the tutorial series from WandB - [How to Fine-tune a LLM](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)\n",
    "\n",
    "This covers:\n",
    "1. Loading FT dataset from WandB\n",
    "2. Load model and freeze bottom layers\n",
    "3. Fine-tune model\n",
    "4. Save model\n",
    "5. Upload metrics and eval samples to WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download processed dataset from WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "from wandb import Api\n",
    "from datasets import load_from_disk # for some reason load_dataset gives an error\n",
    "\n",
    "# load the packed dataset\n",
    "api = Api()\n",
    "artifact = api.artifact('vijaygkd/alpaca_ft/alpaca_with_prompt_gpt4_splitted:v0', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "    \n",
    "train_dataset = load_jsonl(f\"{artifact_dir}/alpaca_with_prompt_gpt4_train.jsonl\")\n",
    "eval_dataset = load_jsonl(f\"{artifact_dir}/alpaca_with_prompt_gpt4_eval.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download packed dataset from WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact packed_alpaca_hf:v0, 178.69MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:2.2\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb import Api\n",
    "from datasets import load_from_disk # for some reason load_dataset gives an error\n",
    "\n",
    "# load the packed dataset\n",
    "api = Api()\n",
    "artifact = api.artifact('capecape/alpaca_ft/packed_alpaca_hf:v0', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "ds_packed = load_from_disk(artifact_dir)\n",
    "\n",
    "# we are back where we started!\n",
    "train_ds_packed = ds_packed[\"train\"]\n",
    "eval_ds_packed  = ds_packed[\"eval\"]\n",
    "max_seq_len = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 22:23:09.456626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 22:23:12.528441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-03-27 22:23:12.528457: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-27 22:23:18.089885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-03-27 22:23:18.090015: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2024-03-27 22:23:18.090026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "batch_size = 32  # I have an A100 GPU with 80GB of RAM üòé\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator üòé\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']),\n",
       " tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937]),\n",
       " tensor([13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889, 14350,\n",
       "           263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009, 29889,\n",
       "            13,    13,  2277, 29937,  2799]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b.keys(), b[\"input_ids\"][0][:25], b[\"labels\"][0][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the labels are left shifted by 1 for this tutorial as we are using simple training loop.LookupError\n",
    "\n",
    "When using huggingface model trainer and using `ModelOutput.loss` the shifting is handled by the model automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(model_id='meta-llama/Llama-2-7b-hf',\n",
       "          dataset_name='alpaca-gpt4',\n",
       "          precision='bf16',\n",
       "          n_freeze=24,\n",
       "          lr=0.0002,\n",
       "          n_eval_samples=10,\n",
       "          max_seq_len=1024,\n",
       "          epochs=2,\n",
       "          gradient_accumulation_steps=1,\n",
       "          batch_size=32,\n",
       "          log_model=False,\n",
       "          mom=0.9,\n",
       "          gradient_checkpointing=True,\n",
       "          freeze_embed=True,\n",
       "          total_train_steps=702)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 32 // batch_size\n",
    "\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_seq_len, # Length of the sequences to pack\n",
    "    epochs=2,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    mom=0.9, # optim param\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ‚ùÑÔ∏è\n",
    ")\n",
    "\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60cd75d121a405eb5f0dab2c0b26082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze model layers to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 7B has 32 transformer layers\n",
    "n_freeze = 24 # you can play with this parameter\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True     # unfreeze last 8 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False)\n",
    "\n",
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})  # <- pytorch changed this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "    \n",
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text from model for evaluation and save to W&B\n",
    "\n",
    "Note: This is super helpful for debugging and to check how the model is aligning with the dataset as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"A simple token level Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)      # default generation config\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({table_name:table})\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    eval_acc = Accuracy()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval_loss\": loss.item(),\n",
    "               \"eval_accuracy\": eval_acc.compute()})\n",
    "    \n",
    "    # generate eval predictions\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/vm-a100-80gb/code/LLM-recipes/fine_tuning/wandb/run-20240327_223714-iubn8cix</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/iubn8cix/workspace' target=\"_blank\">icy-smoke-21</a></strong> to <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/iubn8cix/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/iubn8cix/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eb71c993df4fe69f7e9480dd7bab84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ad261fc7254eff9430515669303537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7953297c24c44898aa2bb030111a6b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Exception in thread SockSrvRdThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/service/server_sock.py\", line 99, in run\n",
      "    sreq = self._sock_client.read_server_request()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 274, in read_server_request\n",
      "    data = self._read_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 248, in _read_packet_bytes\n",
      "    rec = self._extract_packet_bytes()\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 230, in _extract_packet_bytes\n",
      "    assert magic == ord(\"W\")\n",
      "AssertionError\n",
      "Bad pipe message: %s [b'rf\\x10iHd\\xc8\\xadk\\xb8@\\n\\x0b\\x07\\x91\\x0e[@ \\t\\xc8K\\xac\\x15`k\\xe1\\xac\\x12\\xa6\\xf6\\x16+\\x89k\\xd9\\x93\\xbc\\xa4\\xc6\\xd9@\\xfbvD\\xfe\\xc5E\\xd6CK\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08', b'\\x05\\x08\\x06']\n",
      "Bad pipe message: %s [b'\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 {C\\xec\\x08Yn\\xdfy3<\\xe4\\x96Se\\x9c\\t\\x88\\xb9 \\xd4(\\x16']\n",
      "Bad pipe message: %s [b\"\\xba\\x10\\xf0p\\xe6\\xf8\\xe1=-Uc7;\\xad\\x05\\xdf'\\t\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\"]\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b\"\\x97\\x04>\\xe9=\\xc4\\x08+;\\xd5\\xa4?\\xda^'\\x13-Q\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\"]\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b\"\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0\"]\n",
      "Bad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04']\n",
      "Bad pipe message: %s [b'\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\xd0c\\x16L\\x1a]\\xfd$\\xfbg.Z;\\xbd\\x9e,\\x05\\x9f\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x18\\xe4\\x14\\x98\\x01\\x10\\xed\\x19V\\xcf{\\xa5\\xa5\\xfa\\xa4\\xfb\\xc2(\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0']\n",
      "Bad pipe message: %s [b'\\xea\\xc1\\xe2\\r-N]\\xec\\xbf\\xbeE@\\x822\\xc8\\x03\\x14\\xef\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x00']\n",
      "Bad pipe message: %s [b'w\\xd0\\xb03R$l\\xe1\\x96 \\x9a\\x1dR]\\xcc>&R\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b\"\\x92\\x19\\xbd\\xba\\xda\\x1c\\x9b\\xd7!\\xe4\\x8d#_\\x8c\\x18I=f\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\"]\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c64c98c12742308032431e2da22875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700423c18ec04f7f980370295b666286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4123fe32ab514d5ab4365d4024990c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4971c0b5265b4fa9921caa412a7124c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5f4fed101d41b18ff15499c854c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.129 MB of 0.129 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval_loss</td><td>‚ñÉ‚ñà‚ñÅ</td></tr><tr><td>train/accuracy</td><td>‚ñÇ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.7435</td></tr><tr><td>eval_loss</td><td>1.08667</td></tr><tr><td>train/accuracy</td><td>0.79082</td></tr><tr><td>train/global_step</td><td>701</td></tr><tr><td>train/learning_rate</td><td>6e-05</td></tr><tr><td>train/loss</td><td>0.80399</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-smoke-21</strong> at: <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/iubn8cix/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/iubn8cix/workspace</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_223714-iubn8cix/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# validate before training once\n",
    "validate()\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "pbar = tqdm(total=config.total_train_steps)\n",
    "for epoch in range(config.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "            pbar.update(1)\n",
    "    validate()      # validate after each batch\n",
    "pbar.close()\n",
    "\n",
    "# we save the model checkpoint at the end\n",
    "save_model(\n",
    "\tmodel, \n",
    "\tmodel_name=config.model_id.replace(\"/\", \"_\"), \n",
    "\tmodels_folder=\".model/\", log=config.log_model\n",
    ")\n",
    "    \n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
