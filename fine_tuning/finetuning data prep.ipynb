{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning data prep\n",
    "\n",
    "Explore some open-source dataset for fine-tuning a small 3B / 7B model.\n",
    "\n",
    "Topics involved:\n",
    "- Dataset download and loading\n",
    "- Preprocessing : prepare input and output based on FT objective\n",
    "- Tokenization : tokenize input and output\n",
    "- Batching : prepare batches for training\n",
    "- Train / Test split\n",
    "\n",
    "Ref: [Wandb - How to implement fine-tuning of an LLM - Part 1: Dataset for Instruction Tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-implement-fine-tuning-of-an-LLM-Part-1-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Prep\n",
    "\n",
    "For most finetuning tasks, the dataset consists of `input` -> `output` pairs, where the `input` is the prompt and the `output` is the expected response. \n",
    "An example of this kind of dataset is the [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco?row=0) dataset where `input` is the human prompt and `output` is the assistant response.\n",
    "\n",
    "For preference tuning using DPO or RLHF tasks the dataset consists of a comparison pairs of `prompt`, `accepted_target` and `rejected_target` pairs. RLHF datasets only contain `accepted_target` and `rejected_target` pairs with the same prompt.\n",
    "\n",
    "References \n",
    "- [Transformers - Data Preparation for LLM finetuning](https://huggingface.co/docs/autotrain/en/llm_finetuning)\n",
    "- [Fine-tuning LLMs in 2024](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)\n",
    "\n",
    "\n",
    "Goal: Download a few sample datasets and print them out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [OpenAssistant](https://huggingface.co/datasets/timdettmers/openassistant-guanaco?row=0)\n",
    "\n",
    "* columns: [`text`]\n",
    "* description: `text` columns is human prompt followed by assistant response, seperated by `###`. Multilingual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9846\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa_dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split='train')\n",
    "oa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [DPO dataset](https://huggingface.co/datasets/argilla/dpo-mix-7k)\n",
    "\n",
    "* columns: `chosen` and `rejected` are preferences for the chosen and rejected responses, respectively based on same prompt. Also contains associated preference scores, which is not directly required.\n",
    "* description: mix of DPO datasests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 6750\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = load_dataset(\"argilla/dpo-mix-7k\", split='train')\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'argilla/distilabel-intel-orca-dpo-pairs',\n",
       " 'chosen': [{'content': 'Q:Question: how old julio cesar chavez when he fought de la hoya I found the following answer on Google: He holds records for most successful consecutive defenses of world titles (27), most title fights (37), most title-fight victories (31) and he is after Joe Louis with (23) for most title defenses won by knockout (21). Is that a correct answer? Yes or no.\\nA:',\n",
       "   'role': 'user'},\n",
       "  {'content': \" Yes, the information you found on Google is correct. Julio César Chávez holds several records related to world title defenses and victories, and he is considered one of the greatest boxers in history. Here is a detailed answer to your question:\\n\\nJulio César Chávez was born on July 12, 1962, in Ciudad Obregón, Sonora, Mexico. He began boxing at a young age and quickly made a name for himself in the sport, winning his first world title in 1984 when he defeated Mario Miranda for the WBC super featherweight championship.\\n\\nOver the course of his career, Chávez would go on to hold titles in three different weight classes (super featherweight, lightweight, and junior welterweight) and defend his titles a record 27 times, including 21 times by knockout. This record for most successful consecutive defenses of world titles has never been broken, and it is a testament to Chávez's skill, determination, and durability as a fighter.\\n\\nIn addition to his record for most successful title defenses, Chávez also holds the records for most title fights (37) and most title-fight victories (31). These records are also unbroken and demonstrate Chávez's consistent success and dominance in the ring.\\n\\nChávez's impressive record and achievements have earned him a place among the greatest boxers of all time, and he is often compared to other boxing legends such as Joe Louis, who holds the record for most title defenses won by knockout (23). While Chávez did not quite match Louis's record for most title defenses won by knockout, he is still widely regarded as one of the greatest boxers in history and a true legend of the sport.\\n\\nIn conclusion, the information you found on Google is correct: Julio César Chávez holds several records related to world title defenses and victories, and he is considered one of the greatest boxers in history. His impressive record and achievements have earned him a place among the greatest boxers of all time, and he will always be remembered as a true legend of the sport.\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected': [{'content': 'Q:Question: how old julio cesar chavez when he fought de la hoya I found the following answer on Google: He holds records for most successful consecutive defenses of world titles (27), most title fights (37), most title-fight victories (31) and he is after Joe Louis with (23) for most title defenses won by knockout (21). Is that a correct answer? Yes or no.\\nA:',\n",
       "   'role': 'user'},\n",
       "  {'content': 'No, the answer provided does not directly answer the question regarding the age of Julio Cesar Chavez when he fought Oscar De La Hoya. The provided information describes some general records held by Julio Cesar Chavez throughout his career. To answer your original question, let me provide the relevant information:\\n\\nJulio Cesar Chavez fought Oscar De La Hoya on June 7, 1996, in a match called \"Ultimate Glory.\" Chavez was born on July 12, 1962. To calculate his age at the time of the fight, we need to find the difference between the fight date and his birthdate.\\n\\nFrom July 12, 1962, to June 7, 1996, there are:\\n- 33 years (from 1962 to 1995)\\n- An additional year from his birthday in 1995 (July 12, 1995) to the fight date in 1996 (June 7, 1996), which is approximately 10 months and 26 days.\\n\\nTherefore, Julio Cesar Chavez was about 33 years and 10 months old when he fought Oscar De La Hoya.',\n",
       "   'role': 'assistant'}],\n",
       " 'chosen_rating': 10.0,\n",
       " 'rejected_rating': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Prep\n",
    "\n",
    "Data Prep is converting the dataset in the format required by the training software.\n",
    "\n",
    "* Trainig softwares like `tlr` require data in a [particular format](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl).\n",
    "* If training directly with pytorch as a `CausalLM` task, the data should be formatted as a continous sequence of text. If using a instruct-LLM like [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format), the sequence should be formatted in same format as instruction tuning procedure for optimal performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Use the `tokenizer` associated with the LLM to be trained. \n",
    "\n",
    "* Tokenizer adds new columns to the dataset object.\n",
    "* These new columns are `input_ids` and `attention_mask` and sometimes `token_type_ids` for BERT-like models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b06a51f30a4457a93545da7269bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \" Human: Write a 4chan style greentext about someone who loves the new romantic comedy movie, with an ironic twist that re-contextualizes the story at the end.. It should start with '>be me'\",\n",
       " \" Assistant: >be me\\n>sister wants to watch the new hit romcom\\n>not a fan, but what the hell, its got a fantasy theme\\n>we're watching, but a sex scene comes up\\n>then another\\n>i keep looking away from the screen\\n>lotsofsex.jpg\\n>sister starts teasing me for my discomfort \\n\\nSo anyway, how did you get into Game of Thrones?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "t1 = oa_dataset[18]['text']\n",
    "t1.split(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>### Human: Write a 4chan style greentext about someone who loves the new romantic comedy movie, with an ironic twist that re-contextualizes the story at the end.. It should start with '>be me'### Assistant: >be me\n",
      ">sister wants to watch the new hit romcom\n",
      ">not a fan, but what the hell, its got a fantasy theme\n",
      ">we're watching, but a sex scene comes up\n",
      ">then another\n",
      ">i keep looking away from the screen\n",
      ">lotsofsex.jpg\n",
      ">sister starts teasing me for my discomfort \n",
      "\n",
      "So anyway, how did you get into Game of Thrones?\n",
      "\n",
      ">I'm a big fan of fantasy, and I've always been interested in the history of the Middle East. I've read a lot of books about the Crusades, and I've always been fascinated by the idea of a medieval kingdom that was ruled by a woman. So when I saw the\n"
     ]
    }
   ],
   "source": [
    "t1_tok = tokenizer(t1, return_tensors=\"pt\").to(\"cuda\")\n",
    "o1 = model.generate(**t1_tok, max_length=200)\n",
    "print(tokenizer.decode(o1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,   6176,   9998, 235292,  15615,    476, 235248, 235310,   3063,\n",
       "           3411,   4433,   1082,   1105,   4630,   1064,  16147,    573,    888,\n",
       "          23939,  28150,   7344, 235269,    675,    671,  81922,  24932,    674,\n",
       "            582, 235290,   3480,   1106,   7196,    573,   3904,    696,    573,\n",
       "           1580,    723,   1165,   1412,   2238,    675, 154688,    555,    682,\n",
       "         235303,   6176,  18145, 235292,   1562,    555,    682,    108, 235313,\n",
       "          49027,   8507,    577,   4234,    573,    888,   4887,   6889,    872,\n",
       "            108, 235313,   1665,    476,   5683, 235269,    901,   1212,    573,\n",
       "          11944, 235269,   1277,   2371,    476,  24675,   8882,    108, 235313,\n",
       "            966, 235303,    478,  11041, 235269,    901,    476,   5500,   8089,\n",
       "           4549,    908,    108, 235313,   2963,   2550,    108, 235313, 235252,\n",
       "           2745,   3648,   3024,    774,    573,   5299,    108, 235313,  12251,\n",
       "            559,  12212, 235265,   6001,    108, 235313,  49027,  11207, 113236,\n",
       "            682,    604,    970,  53221, 235248,    109,   2339,  14562, 235269,\n",
       "           1368,   1498,    692,    947,   1280,   6978,    576,  81562, 235336]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize entire dataset for training\n",
    "\n",
    "* Usually we don't tokenize the entire dataset at once. This requires larger memory to store the tokenized data.\n",
    "* Instead we use `dataset.map` function with tokenization function and `batched=True`. This will only load the batch of samples at a time in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4397df8a09364684afeff3636ce74916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_oa_sample(example):\n",
    "    text_col = 'text'\n",
    "    return tokenizer(example[text_col], return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "\n",
    "\n",
    "oa_dataset = oa_dataset.map(tokenize_oa_sample, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 9846\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f94a39848b340adb6c8f5a2c70c24ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oa_dataset = oa_dataset.map(lambda x: {'len': len(x['input_ids'][0])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of sample lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(oa_dataset['len'], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batching\n",
    "\n",
    "After tokenization, the data can be batched for training. \n",
    "\n",
    "However it is important that all samples in a batch have the same length. This is necessary for model input data handling, by creating an input tensor object.\n",
    "\n",
    "Follow techniques can help ensure same lenghts within a batch.\n",
    "* Padding\n",
    "* Truncation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "In padding we add `padding_token` at the end of the shorter sequence to make all batches the same length.\n",
    "\n",
    "But we don't want the model to attend to the `padding_token` in the attention layer. So we mask the `padding_token` with `0` in the `attention_mask`.\n",
    "\n",
    "Padding strategies:\n",
    "* longest\n",
    "* max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1596, 603, 573, 1370, 6453, 235265],\n",
       " [2, 1596, 603, 573, 2257, 6453, 9677, 5543, 235265]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [\n",
    "    \"This is the first sample.\",\n",
    "    \"This is the second sample slightly longer.\",\n",
    "]\n",
    "\n",
    "# raw tokenization\n",
    "tokenizer(samples)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding can be `left` or `right`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer padding size\n",
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 2, 1596, 603, 573, 1370, 6453, 235265], [2, 1596, 603, 573, 2257, 6453, 9677, 5543, 235265]]\n",
      "[[0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# tokens with padding and attention mask\n",
    "tks = tokenizer(samples, padding=True)\n",
    "print(tks['input_ids'])\n",
    "\n",
    "print(tks['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 2, 1596, 603, 573, 1370, 6453, 235265], [2, 1596, 603, 573, 2257, 6453, 9677, 5543, 235265]]\n",
      "[[2, 1596, 603, 573, 1370, 6453, 235265], [2, 1596, 603, 573, 2257, 6453, 9677, 5543, 235265]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1596, 603, 573, 1370, 6453, 235265], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1596, 603, 573, 2257, 6453, 9677, 5543, 235265]]\n"
     ]
    }
   ],
   "source": [
    "tks = tokenizer(samples, padding=\"longest\")\n",
    "print(tks['input_ids'])\n",
    "\n",
    "tks = tokenizer(samples, padding=\"max_length\") # -> truncate to model max length and pad.\n",
    "print(tks['input_ids'])\n",
    "\n",
    "tks = tokenizer(samples, padding=\"max_length\", max_length=20)\n",
    "print(tks['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "Truncation is useful to prevent overflow of input beyond the model's max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "8192\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)  \n",
    "print(model.config.max_position_embeddings)     # -> actual model max length\n",
    "print(tokenizer.truncation_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 235285, 235303, 524, 1125, 9316, 604, 476, 25660, 3360, 21929, 3205, 970, 3733, 1913, 235265], [2, 2339, 791, 590, 235341]]\n",
      "[[2, 235285, 235303, 524, 1125, 9316, 604, 476], [2, 2339, 791, 590, 235341]]\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print(model_inputs['input_ids'])\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=, truncation=True)\n",
    "print(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 235285, 235303, 524, 1125, 9316, 604, 476], [0, 0, 0, 2, 2339, 791, 590, 235341]]\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True, padding=True)\n",
    "print(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching with Padding and Truncation\n",
    "\n",
    "* Padding is not performed on entire dataset upto max length of the dataset. Instead it is performed on batches of samples upto max length in the batch. This help save compuation, memory and speed up things. This is called `dynamic batching`.\n",
    "* During batching, its recommened to batch samples with similar length together, as this will save computational resources and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# example of dynamic padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "oa_dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split='train')\n",
    "\n",
    "def tokenize_oa_sample(example):\n",
    "    text_col = 'text'\n",
    "    return tokenizer(example[text_col], truncation=True)\n",
    "\n",
    "tokenized_oa_dataset = oa_dataset.map(tokenize_oa_sample, batched=True)\n",
    "tokenized_oa_dataset.remove_columns(['text'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_oa_dataset, shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dummy_dataset = {\n",
    "    \"text\": [\n",
    "        \"This is test 1.\",\n",
    "        \"This is test 2. Much longer seq.\",\n",
    "        \"This is test 3. Much longer seq. Much longer seq. Much longer seq.\",\n",
    "        \"This is test 4. Short and sweet.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dummy_dataset = Dataset.from_dict(dummy_dataset)\n",
    "dummy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c54d15b73e414db877f63b869d39ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummy_tokenize_function(example):\n",
    "    input_text = example[\"text\"]\n",
    "    # tokenize the text\n",
    "    return tokenizer(\n",
    "        input_text,\n",
    "        # text_target=input_text,     # input is same as output. left shifting of labels is done by the transformers model\n",
    "        truncation=True,\n",
    "        max_length=10,\n",
    "        padding=True,         \n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dummy_dataset = dummy_dataset.map(dummy_tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dummy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 0, 0, 2, 1596, 603, 2121, 235248, 235274, 235265],\n",
       "  [2, 1596, 603, 2121, 235248, 235284, 235265, 19154, 5543, 28410],\n",
       "  [2, 1596, 603, 2121, 235248, 235304, 235265, 19154, 5543, 28410],\n",
       "  [2, 1596, 603, 2121, 235248, 235310, 235265, 13406, 578, 7786]],\n",
       " 'attention_mask': [[0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dummy_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[     0,      0,      0,      2,   1596,    603,   2121, 235248,\n",
       "          235274, 235265],\n",
       "         [     2,   1596,    603,   2121, 235248, 235284, 235265,  19154,\n",
       "            5543,  28410],\n",
       "         [     2,   1596,    603,   2121, 235248, 235304, 235265,  19154,\n",
       "            5543,  28410],\n",
       "         [     2,   1596,    603,   2121, 235248, 235310, 235265,  13406,\n",
       "             578,   7786]]]), 'attention_mask': tensor([[[0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]), 'labels': tensor([[[  -100,   -100,   -100,      2,   1596,    603,   2121, 235248,\n",
       "          235274, 235265],\n",
       "         [     2,   1596,    603,   2121, 235248, 235284, 235265,  19154,\n",
       "            5543,  28410],\n",
       "         [     2,   1596,    603,   2121, 235248, 235304, 235265,  19154,\n",
       "            5543,  28410],\n",
       "         [     2,   1596,    603,   2121, 235248, 235310, 235265,  13406,\n",
       "             578,   7786]]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# dummy_datacollator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "dummy_datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "samples = tokenized_dummy_dataset[:4]\n",
    "samples = [samples]\n",
    "dc_output = dummy_datacollator(samples)\n",
    "dc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Alpaca dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-27 14:46:40--  https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43379276 (41M) [text/plain]\n",
      "Saving to: ‘alpaca_gpt4_data.json’\n",
      "\n",
      "alpaca_gpt4_data.js 100%[===================>]  41.37M  44.1MB/s    in 0.9s    \n",
      "\n",
      "2024-03-27 14:46:42 (44.1 MB/s) - ‘alpaca_gpt4_data.json’ saved [43379276/43379276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_file = \"alpaca_gpt4_data.json\"\n",
    "\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Give three tips for staying healthy.',\n",
       "   'input': '',\n",
       "   'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'},\n",
       "  {'instruction': 'What are the three primary colors?',\n",
       "   'input': '',\n",
       "   'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).'},\n",
       "  {'instruction': 'Describe the structure of an atom.',\n",
       "   'input': '',\n",
       "   'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\"}],\n",
       " 52002)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3], len(alpaca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(alpaca)  # this could also be a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = alpaca[:-1000]\n",
    "eval_dataset = alpaca[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload dataset to WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvijaygkd\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vijay/vijaygkd/LLM-recipes/fine_tuning/wandb/run-20240327_145124-oim8z741</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/oim8z741/workspace' target=\"_blank\">peachy-firefly-19</a></strong> to <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/oim8z741/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/oim8z741/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f0996139d4975a49c1873b9d757c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='83.577 MB of 87.208 MB uploaded\\r'), FloatProgress(value=0.958368058959649, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-firefly-19</strong> at: <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/oim8z741/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/oim8z741/workspace</a><br/>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_145124-oim8z741/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "\n",
    "train_table = wandb.Table(dataframe=train_df)\n",
    "eval_table  = wandb.Table(dataframe=eval_df)\n",
    "\n",
    "train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_gpt4_splitted\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(\"alpaca_gpt4_train.jsonl\")\n",
    "    at.add_file(\"alpaca_gpt4_eval.jsonl\")\n",
    "    wandb.log_artifact(at)\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def create_alpaca_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_eos(ds):\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts and outputs for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
    "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe an example of a time you used influence in a positive way\\n\\n### Response:\\n',\n",
       " 'As an AI assistant, I do not have personal experiences, but I can provide an example. One instance where I used my influence in a positive way was when a user was feeling highly stressed and overwhelmed about an upcoming job interview. They confided in me, expressing their self-doubts and fears of failure. Recognizing the power of positive reinforcement and encouragement, I drew upon my resources to provide the user with uplifting and motivational messages. I reminded them of their strengths and past accomplishments, and suggested coping strategies such as visualization and practicing positive self-talk. Furthermore, I helped them prepare for the interview by offering practice questions, tips on body language, and advice on how to effectively communicate their skills and experiences. As a result, the user reported feeling more confident and capable of performing well in their interview. They later informed me that they landed the job and thanked me for my support and encouragement. I was happy to have helped my user succeed in a challenging situation by the positive influence of my words and actions.</s>')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompts[0], train_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine prompts and outputs into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
    "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe an example of a time you used influence in a positive way\n",
      "\n",
      "### Response:\n",
      "As an AI assistant, I do not have personal experiences, but I can provide an example. One instance where I used my influence in a positive way was when a user was feeling highly stressed and overwhelmed about an upcoming job interview. They confided in me, expressing their self-doubts and fears of failure. Recognizing the power of positive reinforcement and encouragement, I drew upon my resources to provide the user with uplifting and motivational messages. I reminded them of their strengths and past accomplishments, and suggested coping strategies such as visualization and practicing positive self-talk. Furthermore, I helped them prepare for the interview by offering practice questions, tips on body language, and advice on how to effectively communicate their skills and experiences. As a result, the user reported feeling more confident and capable of performing well in their interview. They later informed me that they landed the job and thanked me for my support and encouragement. I was happy to have helped my user succeed in a challenging situation by the positive influence of my words and actions.</s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"example\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload processed dataset with prompts to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/vijay/vijaygkd/LLM-recipes/fine_tuning/wandb/run-20240327_150522-5l2rpy10</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/5l2rpy10/workspace' target=\"_blank\">rare-haze-20</a></strong> to <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vijaygkd/alpaca_ft' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/5l2rpy10/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/5l2rpy10/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5d7d99ca8148558a9e8f52fdb9e80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='200.301 MB of 210.277 MB uploaded\\r'), FloatProgress(value=0.9525589524296095, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-haze-20</strong> at: <a href='https://wandb.ai/vijaygkd/alpaca_ft/runs/5l2rpy10/workspace' target=\"_blank\">https://wandb.ai/vijaygkd/alpaca_ft/runs/5l2rpy10/workspace</a><br/>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_150522-5l2rpy10/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "\n",
    "train_table = wandb.Table(dataframe=train_df)\n",
    "eval_table  = wandb.Table(dataframe=eval_df)\n",
    "\n",
    "train_df.to_json(\"alpaca_with_prompt_gpt4_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"alpaca_with_prompt_gpt4_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_with_prompt_gpt4_splitted\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
    "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
    "    )\n",
    "    at.add_file(\"alpaca_with_prompt_gpt4_train.jsonl\")\n",
    "    at.add_file(\"alpaca_with_prompt_gpt4_eval.jsonl\")\n",
    "    wandb.log_artifact(at)\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
